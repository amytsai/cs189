\documentclass[11pt]{article}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx}

\def\Name{Amy Tsai, Allen Li}  % Your name
\def\Homework{6}%Number of Homework, PUT SOMETHING HERE
\def\Session{Spring 2014}


\title{CS 189 Report for Homework \Homework}
\author{\Name}
\markboth{CS189 Homework \Homework\ \Name}{CS 189 \Session\ Homework \Homework\ \Name}
\pagestyle{myheadings}

\begin{document}
\maketitle

\section{Introduction}

In this project, we implemented Decision Trees, Random Forests, and Adaboosted Decision Trees. We didn't use any external sources other than the papers provided with the homework instructions. 

\section{Derivations}

\subsection{Stochastic Gradient Update for Single Layer Neural Networks}

\paragraph{Mean Squared Error} We want to derive the stochastic gradient update for mean squared error:

\[
	J = \frac{1}{2}\sum_{k=1}^{n_{out}}\left( t_k - y_k \right) ^2
\]

Which expanded out is equivalent to:
\[
	J = \frac{1}{2}\sum_{k=1}^{n_{out}}\left( t_k - \sigma \left( \sum_j W_{jk} x_j + b_k \right) \right) ^2
\]

To find the gradient, we'll find the partial derivative with respect to a single element of the weight matrix, $W_{jk}$. We can think of the biases as weights too, with a constant input of $1$. To find the gradient:

\newcommand{\sfn}{\sigma \left( \sum_j W_{jk} x_j\right)}
\begin{align}
\frac{\partial J}{\partial W_{jk}} &= \frac{1}{2}\frac{\partial}{\partial W_{jk}}\left(
t_k - \sfn
\right) ^2 \\
&= \frac{1}{2} \frac{\partial}{\partial W_{jk}} \left[ t_k^2 - 2t_k\sfn + \sfn^2 \right] \\
&= \frac{1}{2} \frac{\partial}{\partial W_{jk}} \left[- 2t_k\sfn + \sfn^2 \right] \\
\begin{split}
&= \frac{1}{2} \left[
-2t_k \sfn \left( 1 - \sfn \right) x_j \right.+ \\
& \qquad \left.2\sfn \cdot \sfn \left( 1 - \sfn \right)
\right]
\end{split} \\
 \label{eq:partialJ}
\begin{split}
\frac{\partial J}{\partial W_{jk}} &= 
-t_k \sfn \left( 1 - \sfn \right) x_j + \\
& \qquad \sfn \cdot \sfn \left( 1 - \sfn \right)
\end{split}
\end{align}

Using the result found in equation  \eqref{eq:partialJ} we can construct $\nabla J$. The update equation simply changes the weights by some learning alpha $\alpha$:
\[
	W_{new} = W_{old} - \alpha \nabla J \circ W_{old}
\]  

(Where $\circ$ is the pointwise product of matrices. )

\paragraph{Cross-entropy error} We want to derive the update equation for cross-entropy error:
\[
	J = - \sum_{k=1}^{n_{out}}\left[t_k \ln y_k + \left(1- t_k \right)\ln \left( 1- y_k \right) \right]
\]

The update equation is similar to that of mean squared error:
\[
	W_{new} = W_{old} - \alpha \nabla J \circ W_{old}
\]  
\section{Results}

We tested our results by performing cross-validation. \\

We obtained results of varying degrees. Our best result for a single Decision Tree was about 7.5\% error rate. Our best result for Adaboosted decision trees was about 6.8\%. Since Random Forests were the most promising of the three algorithms, we more extensively tested those. Here are some test results from Random Forests: \\

\begin{tabular}{|c|c|c||c|}
\hline
Number of Trees & Samples Per Tree & Attributes Per Node & Error Rate (\%)\\
15 & 500 & 25 & 7.2 \\
15 & 500 & 25 & 6.3 \\
15 & 600 & 20 & 6.1 \\
15 & 400 & 20 & 7.5 \\
21 & 800 & 20 & 6.2 \\
21 & 400 & 20 & 6.9 \\
21 & 600 & 20 & 6.5 \\
21 & 900 & 20 & 6.1 \\
21 & 1100 & 20 & 5.8 \\
21 & 1500 & 20 & 5.82 \\
21 & 1100 & 15 & 5.6 \\
31 & 1100 & 15 & 5.4 \\
31 & 1100 & 17 & 5.67 \\
51 & 1100 & 15 & 5.47 \\
\hline
\end{tabular}
\vspace{11pt}
\\
We found that the last configuration was best and therefore used it in our Kaggle submission. It takes about 90 seconds to generate this forest on a 4th generation Intel i5 dual-core 2.4 GHz processor on OS X. 

\end{document}